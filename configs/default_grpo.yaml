# GRPO training config file
# GRPO Training Configuration
experiment:
  name: "dr-grpo-qwen-gsm8k"
  project: "dr-grpo-qwen"
  output_dir: "dr_grpo_qwen_0.5b"
  seed: 42

model:
  name_or_path: "dillonkn/qwen2.5-0.5b-reasoning-sft"
  use_local_files_only: false

data:
  train:
    files:
      - "R3_math/data/gsm8k_original_train.json"
    dataset_name: "gsm8k"
    dataset_size: null # null for full dataset, or specify number for subset
    shuffle: true

  eval:
    files:
      - "R3_math/data/gsm8k_original_test.json"
    dataset_name: "gsm8k"
    dataset_size: null # null for full dataset, or specify number for subset
    shuffle: false

training:
  # main training parameters
  # per_device_train_batch_size: 8
  # gradient_accumulation_steps: 2
  # num_generations: 16

  # gen-evaluation parameters
  generation_batch_siz: 256 # how many samples to generate at once
  steps_per_generation: 2 # number of steps to take before generating new data
  gradient_accumulation_steps: 128  # how to accumulate gradients
  num_generations: 8    # size of each group

  # generation parameters
  temperature: 0.8
  top_p: 0.9
  max_completion_length: 256  # monitor if doesn't finish

  # training configs
  bf16: true
  gradient_checkpointing: true  # use gradient checkpointing to save memory
  loss_type: "dr_grpo"
  scale_rewards: false          # dr. grpo does not scale rewards by std
  reward_weights: null

  # logging and saving configs
  logging_steps: 10
  save_steps: 1000
  save_total_limit: 2

  # evaluation configs
  eval_strategy: "epoch"

  # vLLM configs
  use_vllm: true
  vllm_mode: "colocate"       # "colocate" occurs on same device
  vllm_gpu_memory_utilization: 0.5 # how much GPU memory to use for vLLM

reward:
  format_reward: 0.2
  correctness_reward: 1.0

distributed:
  rank: 0
  world_size: 1
  local_rank: 0
  master_addr: "localhost"
  master_port: "12355"

wandb:
  enabled: true
  log_model: "checkpoint"
