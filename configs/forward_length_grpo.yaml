# GRPO Training Configuration
experiment:
  name: "forward_length"
  project: "rl_runs"
  output_dir: "outputs/forward_length"
  seed: 42

model:
  name_or_path: "dillonkn/qwen2.5-0.5b-reasoning-sft"
  use_local_files_only: false

data:
  dataset_name: "gsm8k"
  train:
    files:
      - "curricula/forward_length_staged_dataset.json"
    dataset_size: null # null for full dataset, or specify number for subset
    shuffle: true

  eval:
    files:
      - "R3_math/data/gsm8k_original_test.json"
    dataset_name: "gsm8k"
    dataset_size: 100 # null for full dataset, or specify number for subset
    shuffle: false

training:
  # main training parameters
  per_device_train_batch_size: 8 # 4
  num_train_epochs: 6

  # gen-evaluation parameters
  # generation_batch_size: 16 # how many samples to generate at once
  steps_per_generation: 16 # number of steps to take before generating new data
  gradient_accumulation_steps: 1 # 2
  num_generations: 4 # 8

  # generation parameters
  temperature: 0.8
  top_p: 0.9
  max_completion_length: 256  # monitor if doesn't finish

  # training configs
  bf16: true
  gradient_checkpointing: false  # false to expedite training
  loss_type: "dr_grpo"
  scale_rewards: false          # dr. grpo does not scale rewards by std
  reward_weights: null
  disable_dropout: true

  # logging and saving configs
  logging_steps: 150
  save_steps: 1000
  save_total_limit: 2

  # evaluation configs
  eval_strategy: "epoch"
  # eval_steps: 1000
  per_device_eval_batch_size: 16 # large eval batches since no gradients needed
  eval_temperature: 0.0 # 0.0 for deterministic eval
  eval_num_generations: 1 # only one generation per eval step


  # vLLM configs
  use_vllm: true
  vllm_mode: "colocate"
  vllm_gpu_memory_utilization: 0.6 # 0.4
  max_num_batched_tokens: 1024

reward:
  format_reward: 0.2
  correctness_reward: 1.0

distributed:
  rank: 0
  world_size: 1
  local_rank: 0
  master_addr: "localhost"
  master_port: "12355"

wandb:
  enabled: true
  log_model: "checkpoint"
